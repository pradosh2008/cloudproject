#Incremental Load

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from foundationutil import *

## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

#Specify job_name
job_name=args['JOB_NAME']

# specify source table name  and target table name
source_tbl='vmci105_chain_cust'
target_tbl='plus_vmci105_chain_cust'
partitionKeys =['id_chain']
#Specify the data will get load loaded in which folder in target bucket
target_prefix='//plus//crm'
source_prefix='//plus//crm'


# specify primary key and foreign key columns name
map_keyval = {'plus_vmci105_chain_cust_key':['id_chain','id_cust'],'plus_store_key':['id_store']
              ,'plus_vmci019_customer_key':['id_cust'] }	
	
#Build foundation frame  to get existing data into  foundation frame
fdn_hash_df=read_fdn(target_prefix,target_tbl,spark)

#  Clean source  df 
inc_clean_df=clean_sourcedf(source_prefix,source_tbl,spark)
#Add Hash value to incremental df
inc_hash_df=calc_rowhash(inc_clean_df)

# Compare incremental df to foundation to df for  capturing incremental records 
inc_leftjoined_fdn_df=inc_leftjoin_fdn(fdn_hash_df,inc_hash_df)


# Call base frame with incremental frame         
base_sdf=create_base_frame(inc_leftjoined_fdn_df
                            ,map_keyval
                            ,glueContext
                            ,spark
                            ,job_name
                            ,source_tbl
                            ,audit_cols=['foundation_program_nam','foundation_insert_tms'])

status=write_to_sink(base_sdf,target_tbl,partitionKeys,glueContext,target_prefix)

#moving to archive
move_to_archive(status,source_prefix,source_tbl)

job.commit()
